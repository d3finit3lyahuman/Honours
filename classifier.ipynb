{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mrjunos/depression-reddit-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7731\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'we understand that most people who reply immediately to an op with an invitation to talk privately mean only to help but this type of response usually lead to either disappointment or disaster it usually work out quite differently here than when you say pm me anytime in a casual social context we have huge admiration and appreciation for the goodwill and good citizenship of so many of you who support others here and flag inappropriate content even more so because we know that so many of you are struggling yourselves we re hard at work behind the scene on more information and resource to make it easier to give and get quality help here this is just a small start our new wiki page explains in detail why it s much better to respond in public comment at least until you ve gotten to know someone it will be maintained at r depression wiki private contact and the full text of the current version is below summary anyone who while acting a a helper invite or accepts private contact i e pm chat or any kind of offsite communication early in the conversion is showing either bad intention or bad judgement either way it s unwise to trust them pm me anytime seems like a kind and generous offer and it might be perfectly well meaning but unless and until a solid rapport ha been established it s just not a wise idea here are some point to consider before you offer or accept an invitation to communicate privately by posting supportive reply publicly you ll help more people than just the op if your response are of good quality you ll educate and inspire other helper the 9 90 rule http en wikipedia org wiki rule internet culture applies here a much a it doe anywhere else on the internet people who are struggling with serious mental health issue often justifiably have a low tolerance for disappointment and a high level of ever changing emotional need unless the helper is able to make a 00 commitment to be there for them in every way for a long a necessary offering a personal inbox a a resource is likely to do more harm than good this is why mental health crisis line responder usually don t give their name and caller aren t allowed to request specific responder it s much healthier and safer for the caller to develop a relationship with the agency a a whole analogously it s much safer and healthier for our ops to develop a relationship with the community a a whole even trained responder are generally not allowed to work high intensity situation alone it s partly about availability but it s mostly about wider perspective and preventing compassion fatigue if a helper get in over their head with someone whose mental health issue including suicidality which is often comorbid with depression escalate in a pm conversation it s much harder for others including the r depression and r suicidewatch moderator to help contrary to common assumption moderator can t see or police pm in our observation over many year the people who say pm me the most are consistently the one with the least understanding of mental health issue and mental health support we all have gap in our knowledge and in our ability to communicate effectively community input mitigates these limitation there s no reason why someone who s truly here to help would want to hide their response from community scrutiny if helper are concerned about their own privacy keep in mind that self disclosure when used supportively is more about the feeling than the detail and that we have no problem here with the use of alt throwaway account and have no restriction on account age or karma we all know the internet is used by some people to exploit or abuse others these people do want to hide their deceptive and manipulative response from everyone except their victim there are many of them who specifically target those who are vulnerable because of mental health issue if a helper invite an op to talk privately and give them a good supportive experience they ve primed that person to be more vulnerable to abuser this sort of cognitive priming tends to be particularly effective when someone s in a state of mental health crisis when people rely more on heuristic than critical reasoning if ops want to talk privately posting on a wide open anonymous forum like reddit might not be the best option although we don t recommend it we do allow ops to request private contact when asking for support if you want to do this please keep your expectation realistic and to have a careful look at the history of anyone who offer to pm before opening up to them', 'label': 1}\n",
      "{'text': 'welcome to r depression s check in post a place to take a moment and share what is going on and how you are doing if you have an accomplishment you want to talk about these shouldn t be standalone post in the sub a they violate the role model rule but are welcome here or are having a tough time but prefer not to make your own post this is a place you can share our subreddit rule are located in the sidebar you can also always access them at r depression about rule since all of them exist for important safety reason we ask everyone here to read and follow them please click report on any harmful content you see here we always want to know and deal a soon a we can we also have several wikis there for help with finding and giving support r depression wiki what is depression provides guidance about what is and isn t a depressive disorder guidance on the complex nature of the illness that are usually grouped under the depression label and redirect information for common off topic issue r depression wiki giving help offer information on the nature and value of peer support for mental health issue in general and lot of guidance for learning what is and isn t usually helpful in giving peer support ysk that the type of rule violation that we most frequently see interfering with people getting safe and relevant support here are people breaking the private contact rule you should never trust anyone who try to get you into a private conversation in response to a post here see r depression wiki private contact i m here to help post this show that you don t understand the most basic principle of peer support especially selectivity the giving help wiki explains more about this role modelling i e achievement or advice post this is an expert free zone that s what peer support mean rule we know that internet culture celebrate not just bragging about your achievement but bragging about your good intention nothing like that is ever acceptable here content that s more about making a statement or casually polling the sub than seeking personal support or in a comment giving it rule and 0 off topic post about difficult situation or circumstance including interpersonal loss grief sadness anger and other difficult emotion are not mental illness the what is depression wiki ha suggestion for other place to post about these issue which are 00 valid and serious but inappropriate here', 'label': 1}\n",
      "{'text': 'anyone else instead of sleeping more when depressed stay up all night to avoid the next day from coming sooner may be the social anxiety in me but life is so much more peaceful when everyone else is asleep and not expecting thing of you', 'label': 1}\n",
      "{'text': 'i ve kind of stuffed around a lot in my life delaying the inevitable of having to work a job and be a responsible adult and i m but the longest i ve ever held a job wa 9 month it wasn t that i m lazy i wa always doing other thing i enjoy but i know now unemployment ha caused most of my depression recently i just feel utterly hopeless when i think soon enough i ll have to move out on my own in some shitty house working a job i couldn t care le about to me it just seems like the perfect recipe to depression', 'label': 1}\n",
      "{'text': 'sleep is my greatest and most comforting escape whenever i wake up these day the literal very first emotion i feel is just misery and reminding myself of all my problem i can t even have a single second to myself it s like waking up everyday is just welcoming yourself back to hell', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# print first 5 examples\n",
    "train = ds[\"train\"]\n",
    "for i in range(5):\n",
    "    print(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we understand that most people who reply immed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>welcome to r depression s check in post a plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone else instead of sleeping more when depr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i ve kind of stuffed around a lot in my life d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sleep is my greatest and most comforting escap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  we understand that most people who reply immed...      1\n",
       "1  welcome to r depression s check in post a plac...      1\n",
       "2  anyone else instead of sleeping more when depr...      1\n",
       "3  i ve kind of stuffed around a lot in my life d...      1\n",
       "4  sleep is my greatest and most comforting escap...      1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7731 entries, 0 to 7730\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    7731 non-null   object\n",
      " 1   label   7731 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 120.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe has 7731 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataframe has {df.shape[0]} rows and {df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    3900\n",
       "1    3831\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get label distribution\n",
    "label_dist = df['label'].value_counts()\n",
    "\n",
    "# rename the index\n",
    "label_dist = label_dist.rename(index={0: 'Not Depression', 1: 'Depression'})\n",
    "\n",
    "# plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=label_dist.index, y=label_dist.values)\n",
    "plt.title('Label Distribution')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocess_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # remove unnecessary white spaces\n",
    "    text = text.strip()\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text) # tokenize the text\n",
    "    text = [i for i in word_tokens if not i in stop_words]\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text]\n",
    "\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = X_train.apply(prepocess_text)\n",
    "X_val_prep = X_val.apply(prepocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we understand that most people who reply immediately to an op with an invitation to talk privately mean only to help but this type of response usually lead to either disappointment or disaster it usually work out quite differently here than when you say pm me anytime in a casual social context we have huge admiration and appreciation for the goodwill and good citizenship of so many of you who support others here and flag inappropriate content even more so because we know that so many of you are struggling yourselves we re hard at work behind the scene on more information and resource to make it easier to give and get quality help here this is just a small start our new wiki page explains in detail why it s much better to respond in public comment at least until you ve gotten to know someone it will be maintained at r depression wiki private contact and the full text of the current version is below summary anyone who while acting a a helper invite or accepts private contact i e pm chat or any kind of offsite communication early in the conversion is showing either bad intention or bad judgement either way it s unwise to trust them pm me anytime seems like a kind and generous offer and it might be perfectly well meaning but unless and until a solid rapport ha been established it s just not a wise idea here are some point to consider before you offer or accept an invitation to communicate privately by posting supportive reply publicly you ll help more people than just the op if your response are of good quality you ll educate and inspire other helper the 9 90 rule http en wikipedia org wiki rule internet culture applies here a much a it doe anywhere else on the internet people who are struggling with serious mental health issue often justifiably have a low tolerance for disappointment and a high level of ever changing emotional need unless the helper is able to make a 00 commitment to be there for them in every way for a long a necessary offering a personal inbox a a resource is likely to do more harm than good this is why mental health crisis line responder usually don t give their name and caller aren t allowed to request specific responder it s much healthier and safer for the caller to develop a relationship with the agency a a whole analogously it s much safer and healthier for our ops to develop a relationship with the community a a whole even trained responder are generally not allowed to work high intensity situation alone it s partly about availability but it s mostly about wider perspective and preventing compassion fatigue if a helper get in over their head with someone whose mental health issue including suicidality which is often comorbid with depression escalate in a pm conversation it s much harder for others including the r depression and r suicidewatch moderator to help contrary to common assumption moderator can t see or police pm in our observation over many year the people who say pm me the most are consistently the one with the least understanding of mental health issue and mental health support we all have gap in our knowledge and in our ability to communicate effectively community input mitigates these limitation there s no reason why someone who s truly here to help would want to hide their response from community scrutiny if helper are concerned about their own privacy keep in mind that self disclosure when used supportively is more about the feeling than the detail and that we have no problem here with the use of alt throwaway account and have no restriction on account age or karma we all know the internet is used by some people to exploit or abuse others these people do want to hide their deceptive and manipulative response from everyone except their victim there are many of them who specifically target those who are vulnerable because of mental health issue if a helper invite an op to talk privately and give them a good supportive experience they ve primed that person to be more vulnerable to abuser this sort of cognitive priming tends to be particularly effective when someone s in a state of mental health crisis when people rely more on heuristic than critical reasoning if ops want to talk privately posting on a wide open anonymous forum like reddit might not be the best option although we don t recommend it we do allow ops to request private contact when asking for support if you want to do this please keep your expectation realistic and to have a careful look at the history of anyone who offer to pm before opening up to them\n",
      "understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social context huge admiration appreciation goodwill good citizenship many support others flag inappropriate content even know many struggling hard work behind scene information resource make easier give get quality help small start new wiki page explains detail much better respond public comment least gotten know someone maintained r depression wiki private contact full text current version summary anyone acting helper invite accepts private contact e pm chat kind offsite communication early conversion showing either bad intention bad judgement either way unwise trust pm anytime seems like kind generous offer might perfectly well meaning unless solid rapport ha established wise idea point consider offer accept invitation communicate privately posting supportive reply publicly help people op response good quality educate inspire helper rule http en wikipedia org wiki rule internet culture applies much doe anywhere else internet people struggling serious mental health issue often justifiably low tolerance disappointment high level ever changing emotional need unless helper able make commitment every way long necessary offering personal inbox resource likely harm good mental health crisis line responder usually give name caller allowed request specific responder much healthier safer caller develop relationship agency whole analogously much safer healthier ops develop relationship community whole even trained responder generally allowed work high intensity situation alone partly availability mostly wider perspective preventing compassion fatigue helper get head someone whose mental health issue including suicidality often comorbid depression escalate pm conversation much harder others including r depression r suicidewatch moderator help contrary common assumption moderator see police pm observation many year people say pm consistently one least understanding mental health issue mental health support gap knowledge ability communicate effectively community input mitigates limitation reason someone truly help would want hide response community scrutiny helper concerned privacy keep mind self disclosure used supportively feeling detail problem use alt throwaway account restriction account age karma know internet used people exploit abuse others people want hide deceptive manipulative response everyone except victim many specifically target vulnerable mental health issue helper invite op talk privately give good supportive experience primed person vulnerable abuser sort cognitive priming tends particularly effective someone state mental health crisis people rely heuristic critical reasoning ops want talk privately posting wide open anonymous forum like reddit might best option although recommend allow ops request private contact asking support want please keep expectation realistic careful look history anyone offer pm opening\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(X_train_prep[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilroberta-base')\n",
    "model = AutoModel.from_pretrained('distilbert/distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch encode the text\n",
    "batch_size = 32\n",
    "embeddings_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, tokenizer, model, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size].tolist()\n",
    "        # encode the batch of texts\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "        # get the embeddings\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "        # we only need the output of the [CLS] token\n",
    "        batch_embeddings = output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        # add the embeddings to the list\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('depression-cleaned_embeddings_train.npy'):\n",
    "    embeddings_train = np.load('depression-cleaned_embeddings_train.npy')\n",
    "else:\n",
    "    embeddings_train = get_embeddings(X_train_prep, tokenizer, model, device, batch_size)\n",
    "    np.save('depression-cleaned_embeddings_train.npy', embeddings_train)\n",
    "\n",
    "if os.path.exists('depression-cleaned_embeddings_val.npy'):\n",
    "    embeddings_val = np.load('depression-cleaned_embeddings_val.npy')\n",
    "else:\n",
    "    embeddings_val = get_embeddings(X_val_prep, tokenizer, model, device, batch_size)\n",
    "    np.save('depression-cleaned_embeddings_val.npy', embeddings_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6184, 768)\n",
      "(1547, 768)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_train.shape)\n",
    "print(embeddings_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier, in this case  GRU model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC # AUC is the metric we are interested in, this is the area under the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Honours\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define a Dense model for sentence-level embeddings\n",
    "model = Sequential([\n",
    "    # 2 Dense layers with dropout\n",
    "    Dense(128, activation='relu', input_shape=(embeddings_train.shape[1],)),  # Input shape matches the embedding size\n",
    "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.00001),\n",
    "    metrics=['accuracy', AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5087 - auc_26: 0.5058 - loss: 0.7213 - val_accuracy: 0.5094 - val_auc_26: 0.6531 - val_loss: 0.6915\n",
      "Epoch 2/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4990 - auc_26: 0.4927 - loss: 0.6978 - val_accuracy: 0.5094 - val_auc_26: 0.3033 - val_loss: 0.6952\n",
      "Epoch 3/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4920 - auc_26: 0.5035 - loss: 0.6941 - val_accuracy: 0.7421 - val_auc_26: 0.6549 - val_loss: 0.6912\n",
      "Epoch 4/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4804 - auc_26: 0.4783 - loss: 0.6956 - val_accuracy: 0.5094 - val_auc_26: 0.8224 - val_loss: 0.6908\n",
      "Epoch 5/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5114 - auc_26: 0.4942 - loss: 0.6930 - val_accuracy: 0.7253 - val_auc_26: 0.7894 - val_loss: 0.6900\n",
      "Epoch 6/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4999 - auc_26: 0.5009 - loss: 0.6934 - val_accuracy: 0.5094 - val_auc_26: 0.5020 - val_loss: 0.6927\n",
      "Epoch 7/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4980 - auc_26: 0.5020 - loss: 0.6933 - val_accuracy: 0.5611 - val_auc_26: 0.7027 - val_loss: 0.6919\n",
      "Epoch 8/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4995 - auc_26: 0.4874 - loss: 0.6936 - val_accuracy: 0.5094 - val_auc_26: 0.5000 - val_loss: 0.6929\n",
      "Epoch 9/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5027 - auc_26: 0.4937 - loss: 0.6932 - val_accuracy: 0.5094 - val_auc_26: 0.5000 - val_loss: 0.6929\n",
      "Epoch 10/10\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4984 - auc_26: 0.5026 - loss: 0.6932 - val_accuracy: 0.5094 - val_auc_26: 0.5000 - val_loss: 0.6930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ac4b634310>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    embeddings_train, y_train,\n",
    "    validation_data=(embeddings_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of the embeddings_train:  0.26004666\n",
      "Variance of the embeddings_val:  0.2598177\n"
     ]
    }
   ],
   "source": [
    "print(\"Variance of the embeddings_train: \", np.var(embeddings_train))\n",
    "print(\"Variance of the embeddings_val: \", np.var(embeddings_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4932 - auc_20: 0.4878 - loss: 0.6956 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6955 - learning_rate: 0.2000\n",
      "Epoch 2/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5014 - auc_20: 0.4899 - loss: 0.6955 - val_accuracy: 0.5271 - val_auc_20: 0.5000 - val_loss: 0.7168 - learning_rate: 0.2000\n",
      "Epoch 3/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4986 - auc_20: 0.4931 - loss: 0.7008 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6935 - learning_rate: 0.2000\n",
      "Epoch 4/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4954 - auc_20: 0.4917 - loss: 0.6963 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.7098 - learning_rate: 0.2000\n",
      "Epoch 5/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4872 - auc_20: 0.4730 - loss: 0.6999 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.7005 - learning_rate: 0.2000\n",
      "Epoch 6/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5031 - auc_20: 0.4908 - loss: 0.6965 - val_accuracy: 0.5271 - val_auc_20: 0.5000 - val_loss: 0.6925 - learning_rate: 0.2000\n",
      "Epoch 7/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5067 - auc_20: 0.4982 - loss: 0.6947 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6993 - learning_rate: 0.2000\n",
      "Epoch 8/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5011 - auc_20: 0.4938 - loss: 0.7002 - val_accuracy: 0.5271 - val_auc_20: 0.5000 - val_loss: 0.6927 - learning_rate: 0.2000\n",
      "Epoch 9/20\n",
      "\u001b[1m143/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4941 - auc_20: 0.4848 - loss: 0.6977\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4945 - auc_20: 0.4855 - loss: 0.6976 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6995 - learning_rate: 0.2000\n",
      "Epoch 10/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5023 - auc_20: 0.4942 - loss: 0.6939 - val_accuracy: 0.5271 - val_auc_20: 0.5000 - val_loss: 0.6924 - learning_rate: 0.0200\n",
      "Epoch 11/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4844 - auc_20: 0.4825 - loss: 0.6938 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6945 - learning_rate: 0.0200\n",
      "Epoch 12/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5129 - auc_20: 0.4935 - loss: 0.6930 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6945 - learning_rate: 0.0200\n",
      "Epoch 13/20\n",
      "\u001b[1m151/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5090 - auc_20: 0.4978 - loss: 0.6932\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5089 - auc_20: 0.4978 - loss: 0.6933 - val_accuracy: 0.5271 - val_auc_20: 0.5000 - val_loss: 0.6925 - learning_rate: 0.0200\n",
      "Epoch 14/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4973 - auc_20: 0.4997 - loss: 0.6933 - val_accuracy: 0.5271 - val_auc_20: 0.5000 - val_loss: 0.6931 - learning_rate: 0.0020\n",
      "Epoch 15/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4980 - auc_20: 0.4997 - loss: 0.6932 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6933 - learning_rate: 0.0020\n",
      "Epoch 16/20\n",
      "\u001b[1m141/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4747 - auc_20: 0.4990 - loss: 0.6932\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001999999862164259.\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4757 - auc_20: 0.4987 - loss: 0.6932 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6935 - learning_rate: 0.0020\n",
      "Epoch 17/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4996 - auc_20: 0.5000 - loss: 0.6932 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6935 - learning_rate: 2.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5185 - auc_20: 0.5000 - loss: 0.6929 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6935 - learning_rate: 2.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m151/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5055 - auc_20: 0.5000 - loss: 0.6931\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.9999998039565982e-05.\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5054 - auc_20: 0.5000 - loss: 0.6931 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6935 - learning_rate: 2.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4913 - auc_20: 0.5000 - loss: 0.6933 - val_accuracy: 0.4729 - val_auc_20: 0.5000 - val_loss: 0.6935 - learning_rate: 2.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ac48316110>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(embeddings_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[lr_scheduler])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
